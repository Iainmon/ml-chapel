{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "l1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3)\n",
    "d = torch.randn(1,28,28)\n",
    "l1(d).shape\n",
    "\n",
    "l2 = nn.MaxPool2d(kernel_size=2)\n",
    "l2(l1(d)).shape\n",
    "\n",
    "l3 = nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3)\n",
    "l3(l2(l1(d))).shape\n",
    "\n",
    "l4 = nn.MaxPool2d(kernel_size=2)\n",
    "l4(l3(l2(l1(d)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_data.data[5], cmap='gray')\n",
    "plt.title('%i' % train_data.targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  13,  25, 100, 122,   7,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  33,\n",
       "         151, 208, 252, 252, 252, 146,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 152, 244,\n",
       "         252, 253, 224, 211, 252, 232,  40,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  15, 152, 239, 252, 252,\n",
       "         252, 216,  31,  37, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  96, 252, 252, 252, 252,\n",
       "         217,  29,   0,  37, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 181, 252, 252, 220, 167,\n",
       "          30,   0,   0,  77, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 128,  58,  22,   0,\n",
       "           0,   0,   0, 100, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 157, 252, 252,  60,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 110,\n",
       "         121, 122, 121, 202, 252, 194,   3,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  53, 179, 253,\n",
       "         253, 255, 253, 253, 228,  35,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   5,  54, 227, 252, 243, 228,\n",
       "         170, 242, 252, 252, 231, 117,   6,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   6,  78, 252, 252, 125,  59,   0,\n",
       "          18, 208, 252, 252, 252, 252,  87,   7,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   5, 135, 252, 252, 180,  16,   0,  21,\n",
       "         203, 253, 247, 129, 173, 252, 252, 184,  66,  49,  49,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   3, 136, 252, 241, 106,  17,   0,  53, 200,\n",
       "         252, 216,  65,   0,  14,  72, 163, 241, 252, 252, 223,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 105, 252, 242,  88,  18,  73, 170, 244, 252,\n",
       "         126,  29,   0,   0,   0,   0,   0,  89, 180, 180,  37,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 231, 252, 245, 205, 216, 252, 252, 252, 124,\n",
       "           3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 207, 252, 252, 252, 252, 178, 116,  36,   4,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  13,  93, 143, 121,  23,   6,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2a0477a50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)\n",
    "loss_func = nn.CrossEntropyLoss()   \n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.01)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.1285\n",
      "Epoch [1/10], Step [200/600], Loss: 0.0850\n",
      "Epoch [1/10], Step [300/600], Loss: 0.0448\n",
      "Epoch [1/10], Step [400/600], Loss: 0.0912\n",
      "Epoch [1/10], Step [500/600], Loss: 0.0543\n",
      "Epoch [1/10], Step [600/600], Loss: 0.0556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train(num_epochs, cnn, loaders)\n",
      "Cell \u001b[0;32mIn[23], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, cnn, loaders)\u001b[0m\n\u001b[1;32m     14\u001b[0m b_x \u001b[39m=\u001b[39m Variable(images)   \u001b[39m# batch x\u001b[39;00m\n\u001b[1;32m     15\u001b[0m b_y \u001b[39m=\u001b[39m Variable(labels)   \u001b[39m# batch y\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output \u001b[39m=\u001b[39m cnn(b_x)[\u001b[39m0\u001b[39m]               \n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_func(output, b_y)\n\u001b[1;32m     19\u001b[0m \u001b[39m# clear gradients for this training step   \u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> 24\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     25\u001b[0m     \u001b[39m# flatten the output of conv2 to (batch_size, 32 * 7 * 7)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)       \n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "num_epochs = 10\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    cnn.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "                pass\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass\n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist\n",
    "train_images = mnist.train_images()[:1000]\n",
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
